{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvBBLSbNJ3wU"
      },
      "outputs": [],
      "source": [
        "%pip install requests\n",
        "%pip install bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFjfL7gAho1l",
        "outputId": "513dee53-26fd-4ad5-efd2-52022c337f2b"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "url = \"https://en.wikipedia.org/wiki/World_War_II\"\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content of the webpage\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    # Extract text data from the webpage (customize this based on the webpage's structure)\n",
        "    webpage_text = \" \".join([p.get_text() for p in soup.find_all(\"p\")])\n",
        "else:\n",
        "    print(\"Failed to fetch webpage content\")\n",
        "\n",
        "\n",
        "print(webpage_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4N_PFs2hpEK",
        "outputId": "b7addf54-f89d-42ab-cf1c-c12f9ec6fc4d"
      },
      "outputs": [],
      "source": [
        "\n",
        "%pip install transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niwUpiadmS5J"
      },
      "outputs": [],
      "source": [
        "with open(\"webpage_data.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(webpage_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23Nf1DCYOhNV",
        "outputId": "afee5ced-c347-4389-d49f-fac91be24170"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "# Sample input text\n",
        "# input_text = \"Hello, this is an example sentence with some stop words and punctuation!\"\n",
        "\n",
        "# Remove HTML tags (if applicable)\n",
        "# Remove punctuation, convert to lowercase, and tokenize\n",
        "# input_text = BeautifulSoup(input_text, 'html.parser').get_text()\n",
        "# input_text = webpage_text.lower()\n",
        "# input_text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", input_text)\n",
        "# tokens = word_tokenize(input_text)\n",
        "\n",
        "# # Remove stop words\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "# filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# # Stemming (optional)\n",
        "# stemmer = PorterStemmer()\n",
        "# stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "# # Join tokens back into a sentence or use as needed\n",
        "# processed_text = ' '.join(stemmed_tokens)\n",
        "# print(processed_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZzSDnBL0afM",
        "outputId": "0bb42776-6c81-4460-94b5-371e982124e0"
      },
      "outputs": [],
      "source": [
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define the input and output file paths\n",
        "input_file_path = \"webpage_data.txt\"  # Replace with your input file path\n",
        "output_file_path = \"tokenized_webpage_data.txt\"  # Replace with your output file path\n",
        "\n",
        "# Initialize the sentence tokenizer\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "# Read the input text from the file\n",
        "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "    input_text = input_file.read()\n",
        "\n",
        "input_text = input_text.lower()\n",
        "input_text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", input_text)\n",
        "tokens = word_tokenize(input_text)\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# Stemming (optional)\n",
        "# stemmer = PorterStemmer()\n",
        "# stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "# Join tokens back into a sentence or use as needed\n",
        "processed_text = ' '.join(filtered_tokens)\n",
        "# print(processed_text)\n",
        "input_text=processed_text\n",
        "\n",
        "# Tokenize the input text into sentences\n",
        "sentences = tokenizer.tokenize(input_text)\n",
        "\n",
        "# Write each sentence to the output file on a new line\n",
        "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "    for sentence in sentences:\n",
        "        output_file.write(sentence.strip() + '\\n')\n",
        "\n",
        "# import re\n",
        "# re.sub(r'[^a-zA-Z0-9\\s]', '', output_file_path) #Removing special characters\n",
        "\n",
        "print(f\"Sentences tokenized and saved to '{output_file_path}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDN8GixXmtI0"
      },
      "outputs": [],
      "source": [
        "%pip install transformers[torch,accelerate]>=0.20.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Idr96hK0W-nz"
      },
      "outputs": [],
      "source": [
        "dict_faq=\"dict_faq.txt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "PzVk6UW7kKAl",
        "outputId": "cd68ede4-79bd-44f7-affa-52d7f89b1a77"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n",
        "torch.utils.bottleneck = None\n",
        "\n",
        "model_name = \"gpt2\"  # You can choose a different model if needed\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name).to('cuda:0')  # Move to GPU\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# from transformers import RobertaForCausalLM, RobertaTokenizer\n",
        "\n",
        "# # Load the RoBERTa model and tokenizer\n",
        "# model_name = \"roberta\"\n",
        "# model = RobertaForCausalLM.from_pretrained(\"roberta\").to('cuda:0')  # Move to GPU\n",
        "# tokenizer = RobertaTokenizer.from_pretrained(\"roberta\")\n",
        "\n",
        "\n",
        "# Collect and preprocess your data (replace with actual data collection and preprocessing)\n",
        "data = \"tokenized_webpage_data.txt\"\n",
        "\n",
        "# Tokenize and format the data for training\n",
        "train_dataset = TextDataset(tokenizer=tokenizer, file_path=data, block_size=128)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine-tuned-model1\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=120,\n",
        "    per_device_train_batch_size=50,\n",
        "    save_steps=10000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Prepare the data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False,\n",
        ")\n",
        "\n",
        "# Create a Trainer and fine-tune the model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-uK_7A4o3j_",
        "outputId": "ae5fa737-6877-4a3d-98af-2aa0476f657b"
      },
      "outputs": [],
      "source": [
        "\n",
        "user_prompt =  \"How can I calculate my Grade Point Average?\"\n",
        "\n",
        "ft_model_name=\"./fine-tuned-model1\"\n",
        "ft_model = GPT2LMHeadModel.from_pretrained(ft_model_name).to('cuda:0')\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(ft_model_name)\n",
        "user_prompt_tokens = tokenizer.encode(user_prompt, return_tensors=\"pt\").to('cuda:0')\n",
        "attention_mask = torch.ones(user_prompt_tokens.shape, dtype=user_prompt_tokens.dtype, device=user_prompt_tokens.device)\n",
        "\n",
        "\n",
        "response =model.generate(\n",
        "    input_ids=user_prompt_tokens,\n",
        "    max_length=50,  # Adjust the maximum response length as needed\n",
        "    num_return_sequences=1,\n",
        "    no_repeat_ngram_size=3,\n",
        "    pad_token_id=tokenizer.eos_token_id,  # Set pad_token_id explicitly\n",
        "    attention_mask=attention_mask\n",
        ")\n",
        "\n",
        "chatbot_response = tokenizer.decode(response[0], skip_special_tokens=True)\n",
        "print(\"Chatbot:\\n\", chatbot_response)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
